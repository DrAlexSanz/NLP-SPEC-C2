{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Copia de Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKQsoViInGgJM+lyQ/9MKl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/NLP-SPEC-C2/blob/master/W2/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx_bCmQGA5lf"
      },
      "source": [
        "#Assignment 2: Parts-of-Speech Tagging (POS)\n",
        "\n",
        "Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective...) to each word in an input text. Tagging is difficult because some words can represent more than one part of speech at different times. They are Ambiguous. Let's look at the following example:\n",
        "\n",
        "* The whole team played well. [adverb]\n",
        "* You are doing well for yourself. [adjective]\n",
        "* Well, this assignment took me forever to complete. [interjection]\n",
        "* The well is dry. [noun]\n",
        "* Tears were beginning to well in her eyes. [verb]\n",
        "\n",
        "Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will:\n",
        "\n",
        "Learn how parts-of-speech tagging works\n",
        "* Compute the transition matrix A in a Hidden Markov Model\n",
        "* Compute the transition matrix B in a Hidden Markov Model\n",
        "* Compute the Viterbi algorithm\n",
        "* Compute the accuracy of your own model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4DRIGMH1HH7",
        "outputId": "1238a8d6-7761-47c0-9544-1f894f811155",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "# Download everything:\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_02-21.pos\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_24.pos\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/hmm_vocab.txt\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/test.words.txt\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/utils_pos.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-15 16:20:27--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_02-21.pos\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8279089 (7.9M) [text/plain]\n",
            "Saving to: ‘WSJ_02-21.pos’\n",
            "\n",
            "WSJ_02-21.pos       100%[===================>]   7.90M  16.3MB/s    in 0.5s    \n",
            "\n",
            "2020-10-15 16:20:29 (16.3 MB/s) - ‘WSJ_02-21.pos’ saved [8279089/8279089]\n",
            "\n",
            "--2020-10-15 16:20:29--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_24.pos\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 286063 (279K) [text/plain]\n",
            "Saving to: ‘WSJ_24.pos’\n",
            "\n",
            "WSJ_24.pos          100%[===================>] 279.36K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-10-15 16:20:29 (4.64 MB/s) - ‘WSJ_24.pos’ saved [286063/286063]\n",
            "\n",
            "--2020-10-15 16:20:29--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/hmm_vocab.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 196571 (192K) [text/plain]\n",
            "Saving to: ‘hmm_vocab.txt’\n",
            "\n",
            "hmm_vocab.txt       100%[===================>] 191.96K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2020-10-15 16:20:30 (4.14 MB/s) - ‘hmm_vocab.txt’ saved [196571/196571]\n",
            "\n",
            "--2020-10-15 16:20:30--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/test.words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180266 (176K) [text/plain]\n",
            "Saving to: ‘test.words.txt’\n",
            "\n",
            "test.words.txt      100%[===================>] 176.04K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-10-15 16:20:30 (4.01 MB/s) - ‘test.words.txt’ saved [180266/180266]\n",
            "\n",
            "--2020-10-15 16:20:31--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/utils_pos.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2455 (2.4K) [text/plain]\n",
            "Saving to: ‘utils_pos.py’\n",
            "\n",
            "utils_pos.py        100%[===================>]   2.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-15 16:20:31 (19.6 MB/s) - ‘utils_pos.py’ saved [2455/2455]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Xrpwsh1MC5",
        "outputId": "b89c4245-833c-4e18-bae2-5d588b248e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils_pos import get_word_tag, preprocess\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"Everything imported correctly\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything imported correctly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4IaRVTYBqgL"
      },
      "source": [
        "## Part 0: Data Sources\n",
        "This assignment will use two tagged data sets collected from the Wall Street Journal (WSJ).\n",
        "\n",
        "One data set (WSJ-2_21.pos) will be used for training.\n",
        "\n",
        "The other (WSJ-24.pos) for testing.\n",
        "\n",
        "The tagged training data has been preprocessed to form a vocabulary (hmm_vocab.txt).\n",
        "\n",
        "The words in the vocabulary are words from the training set that were used two or more times.\n",
        "\n",
        "The vocabulary is augmented with a set of 'unknown word tokens', described below.\n",
        "\n",
        "The training set will be used to create the emission, transmission and tag counts.\n",
        "\n",
        "The test set (WSJ-24.pos) is read in to create y.\n",
        "\n",
        "This contains both the test text and the true tag.\n",
        "The test set has also been preprocessed to remove the tags to form test_words.txt.\n",
        "\n",
        "This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in utils_pos.py.\n",
        "\n",
        "This forms the list prep, the preprocessed text used to test our POS taggers.\n",
        "A POS tagger will necessarily encounter words that are not in its datasets.\n",
        "\n",
        "To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag.\n",
        "\n",
        "For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'.\n",
        "\n",
        "A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n",
        "\n",
        "Implementation note:\n",
        "\n",
        "* For python 3.6 and beyond, dictionaries retain the insertion order.\n",
        "* Furthermore, their hash-based lookup makes them suitable for rapid membership tests.\n",
        "  * If di is a dictionary, key in di will return True if di has a key key, else False.\n",
        "* The dictionary vocab will utilize these features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfQFZvVqCC0x",
        "outputId": "3232be2b-d919-454b-d240-7ee24f495d42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load training corpus\n",
        "\n",
        "with open(\"WSJ_02-21.pos\", \"r\") as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "print(f\"First 5 lines of the training corpus:\", training_corpus[0:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 lines of the training corpus: ['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2zyMmg3CpLD",
        "outputId": "44ec5615-22d3-401e-c6d2-0482efabfd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Read the vocabulary data, split by word (line) and save the vocab\n",
        "\n",
        "with open(\"hmm_vocab.txt\") as f:\n",
        "    voc_list = f.read().split(\"\\n\")\n",
        "\n",
        "print(f\"The first 5 words\", voc_list[0:5])\n",
        "\n",
        "print(f\"The last 5 words\", voc_list[-5:])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first 5 words ['!', '#', '$', '%', '&']\n",
            "The last 5 words ['zones', 'zoning', '{', '}', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PFCxKFJDaH8",
        "outputId": "8e8e9d57-a7a4-4f04-b328-38471364c4e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# Now make a dictionary of the words in vocab\n",
        "\n",
        "vocab = {}\n",
        "\n",
        "for i, word in enumerate(sorted(voc_list)):\n",
        "    vocab[word] = i\n",
        "\n",
        "print(\"Vocabulary dictionary. Key is the word and Value is a unique integer (ID), not a count.\")\n",
        "\n",
        "for k, v in vocab.items():\n",
        "    print(f\"{k}:{v}\")\n",
        "\n",
        "    if v > 10:\n",
        "        break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary dictionary. Key is the word and Value is a unique integer (ID), not a count.\n",
            ":0\n",
            "!:1\n",
            "#:2\n",
            "$:3\n",
            "%:4\n",
            "&:5\n",
            "':6\n",
            "'':7\n",
            "'40s:8\n",
            "'60s:9\n",
            "'70s:10\n",
            "'80s:11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYtFdg6JEsaM",
        "outputId": "64aebd43-bdd2-4dd6-ab50-f9fa39c8bd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load test corpus\n",
        "\n",
        "with open(\"WSJ_24.pos\", \"r\") as f:\n",
        "    test_corpus = f.readlines()\n",
        "\n",
        "print(f\"The first 5 lines of the test corpus:\", test_corpus[0:5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first 5 lines of the test corpus: ['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGRai2XHFGGP"
      },
      "source": [
        "# Separate tags from test data\n",
        "\n",
        "_, prep = preprocess(vocab, \"test.words.txt\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzywEMjSFcQU",
        "outputId": "40a8c0bb-2c97-4aee-cb30-609bd18fd57f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(prep[0:10])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfsTEMQOFjxD"
      },
      "source": [
        "# Part 1: Parts-of-speech tagging\n",
        "\n",
        "## Part 1.1 - Training\n",
        "You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art.\n",
        "\n",
        "In this section, you will find the words that are not ambiguous.\n",
        "\n",
        "* For example, the word is is a verb and it is not ambiguous.\n",
        "* In the WSJ corpus, $86$% of the token are unambiguous (meaning they have only one tag)\n",
        "* About $14\\%$ are ambiguous (meaning that they have more than one tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q127qrxF4-r"
      },
      "source": [
        "Before you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables.\n",
        "\n",
        "Transition counts\n",
        "The first dictionary is the transition_counts dictionary which computes the number of times each tag happened next to another tag.\n",
        "This dictionary will be used to compute:$$P(t_i |t_{i-1}) \\tag{1}$$\n",
        "\n",
        "This is the probability of a tag at position $i$ given the tag at position $i-1$.\n",
        "\n",
        "In order for you to compute equation 1, you will create a transition_counts dictionary where\n",
        "\n",
        "The keys are (prev_tag, tag)\n",
        "The values are the number of times those two tags appeared in that order.\n",
        "Emission counts\n",
        "The second dictionary you will compute is the emission_counts dictionary. This dictionary will be used to compute:\n",
        "\n",
        "$$P(w_i|t_i)\\tag{2}$$\n",
        "In other words, you will use it to compute the probability of a word given its tag.\n",
        "\n",
        "In order for you to compute equation 2, you will create an emission_counts dictionary where\n",
        "\n",
        "The keys are (tag, word)\n",
        "The values are the number of times that pair showed up in your training set.\n",
        "Tag counts\n",
        "The last dictionary you will compute is the tag_counts dictionary.\n",
        "\n",
        "The key is the tag\n",
        "The value is the number of times each tag appeared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvQkpjRfF_pm"
      },
      "source": [
        "## Exercise 01\n",
        "**Instructions:** Write a program that takes in the training_corpus and returns the three dictionaries mentioned above transition_counts, emission_counts, and tag_counts.\n",
        "\n",
        "**emission_counts:** maps (tag, word) to the number of times it happened.\n",
        "\n",
        "**transition_counts:** maps (prev_tag, tag) to the number of times it has appeared.\n",
        "\n",
        "**tag_counts:** maps (tag) to the number of times it has occured.\n",
        "\n",
        "Implementation note: This routine utilises defaultdict, which is a subclass of dict.\n",
        "\n",
        "* A standard Python dictionary throws a KeyError if you try to access an item with a key that is not currently in the dictionary.\n",
        "* In contrast, the defaultdict will create an item of the type of the argument, in this case an integer with the default value of 0.\n",
        "* See defaultdict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKBh_SaQAz0i"
      },
      "source": [
        "def create_dicts(training_corpus, vocab):\n",
        "    \"\"\"\n",
        "    Training corpus is an object with a word and a tag per line\n",
        "    Vocab: Dictionary with the words and a unique key\n",
        "\n",
        "    Output:\n",
        "    Emission counts: a dict where the keys are the tuples (tag, word) and the values are the counts\n",
        "    transition counts: a dict where the keys are the tuples (prev_tag, current_tag) and the values are the number of times this happens.\n",
        "    tag counts: another dict where the keys are the tags and the values are the counts.\n",
        "    \"\"\"\n",
        "\n",
        "    # First initialize dicts\n",
        "\n",
        "    emmision_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "\n",
        "    # I'm going to initialize the previous tag as the start token <S>\n",
        "\n",
        "    prev_tag = \"<S>\"\n",
        "\n",
        "    # Also use i to count the number of lines in training_corpus\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for word_tag in training_corpus:\n",
        "\n",
        "        i += 1\n",
        "\n",
        "        if i % 50000 == 0:\n",
        "            print(\"Number of lines processed:\", i)\n",
        "\n",
        "        word, tag = get_word_tag(word_tag, vocab)\n",
        "\n",
        "        # Now increment the prev/current tag count\n",
        "\n",
        "        emmision_counts[(tag, word)] += 1\n",
        "        \n",
        "        transition_counts[(prev_tag, tag)] += 1\n",
        "\n",
        "        tag_counts[tag] += 1\n",
        "\n",
        "        prev_tag = tag\n",
        "\n",
        "    return emmision_counts, transition_counts, tag_counts\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcH6L_bDI8hN",
        "outputId": "16453800-f70b-4474-fd4d-40c090cfe8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "emmision_counts, transition_counts, tag_counts = create_dicts(training_corpus, vocab)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of lines processed: 50000\n",
            "Number of lines processed: 100000\n",
            "Number of lines processed: 150000\n",
            "Number of lines processed: 200000\n",
            "Number of lines processed: 250000\n",
            "Number of lines processed: 300000\n",
            "Number of lines processed: 350000\n",
            "Number of lines processed: 400000\n",
            "Number of lines processed: 450000\n",
            "Number of lines processed: 500000\n",
            "Number of lines processed: 550000\n",
            "Number of lines processed: 600000\n",
            "Number of lines processed: 650000\n",
            "Number of lines processed: 700000\n",
            "Number of lines processed: 750000\n",
            "Number of lines processed: 800000\n",
            "Number of lines processed: 850000\n",
            "Number of lines processed: 900000\n",
            "Number of lines processed: 950000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58DYIzdXJme8",
        "outputId": "c2eddd60-d286-4299-c64e-5b323e3b0f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "states = sorted(tag_counts.keys())\n",
        "print(\"Number of POS states:\", len(states))\n",
        "print(\"Examples of emmision counts:\\n\", list(emmision_counts.items())[0:5])\n",
        "print(\"Examples of transition_counts:\\n\", list(transition_counts.items())[0:5])\n",
        "print(\"Examples of tag counts:\\n\", list(tag_counts.items())[0:5])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of POS states: 46\n",
            "Examples of emmision counts:\n",
            " [(('IN', 'In'), 1735), (('DT', 'an'), 3142), (('NNP', 'Oct.'), 317), (('CD', '19'), 100), (('NN', 'review'), 36)]\n",
            "Examples of transition_counts:\n",
            " [(('<S>', 'IN'), 1), (('IN', 'DT'), 32364), (('DT', 'NNP'), 9044), (('NNP', 'CD'), 1752), (('CD', 'NN'), 7377)]\n",
            "Examples of tag counts:\n",
            " [('IN', 98554), ('DT', 81842), ('NNP', 91466), ('CD', 36568), ('NN', 132935)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRd8GfYHK6ve"
      },
      "source": [
        "## Part 1.2 - Testing\n",
        "Now you will test the accuracy of your parts-of-speech tagger using your emission_counts dictionary.\n",
        "\n",
        "Given your preprocessed test corpus prep, you will assign a parts-of-speech tag to every word in that corpus.\n",
        "Using the original tagged test corpus y, you will then compute what percent of the tags you got correct.\n",
        "\n",
        "### Exercise 02\n",
        "**Instructions:** Implement predict_pos that computes the accuracy of your model.\n",
        "\n",
        "This is a warm up exercise.\n",
        "\n",
        "To assign a part of speech to a word, assign the most frequent POS for that word in the training set.\n",
        "Then evaluate how well this approach works. Each time you predict based on the most frequent POS for the given word, check whether the actual POS of that word is the same. If so, the prediction was correct!\n",
        "Calculate the accuracy as the number of correct predictions divided by the total number of words for which you predicted the POS tag."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHzsoxoiRUvy"
      },
      "source": [
        "def predict_pos(prep, y, emmision_counts, vocab, states):\n",
        "    \"\"\"\n",
        "    inputs:\n",
        "    prep: a preprocessed version of y (list with the word part of tuples)\n",
        "    y: ground truth labels\n",
        "    emmision counts: a dict where the keys are the tuples (tag, word) and the values are the counts\n",
        "    vocab: a dictionary where keys are words in vocabulary and value is a unique ID\n",
        "    states: sorted list of all possible tags (the 46)\n",
        "\n",
        "    Output:\n",
        "    The accuracy of the predictions\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize correct\n",
        "    correct = 0\n",
        "    # Get all tuples as a set, the tuples are tag, word\n",
        "    all_words = set(emmision_counts.keys())\n",
        "\n",
        "    # Count the total amount of words to predict\n",
        "\n",
        "    total = len(y)\n",
        "\n",
        "    for word, y_tup in zip(prep, y):\n",
        "\n",
        "        # Split the word and tag into a list of two items\n",
        "\n",
        "        y_tup_list = y_tup.split()\n",
        "        \n",
        "        # Verify that I got two elements in this list\n",
        "        if len(y_tup_list) == 2:\n",
        "            true_label = y_tup_list[1]\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        # Initialize final count\n",
        "        count_final = 0\n",
        "        tag_final = \"\"\n",
        "\n",
        "        if word in vocab:\n",
        "            for tag in states:\n",
        "                # Define the key as the tuple containing POS tag and word\n",
        "\n",
        "                key = (tag, word)\n",
        "\n",
        "                # Check if the key exists in emmision counts (I will put this tag then)\n",
        "\n",
        "                if key in emmision_counts:\n",
        "                    count = emmision_counts[key]\n",
        "\n",
        "                    if count > count_final:\n",
        "                        count_final = count\n",
        "                        tag_final = tag\n",
        "\n",
        "            # If the final tag (highest count) matches the true, I count it as correct\n",
        "            if tag_final == true_label:\n",
        "                correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    return accuracy\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlSFsciyWXGd",
        "outputId": "1d4a4a40-af5b-4cb7-8afd-e4b7be64a614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "acc = predict_pos(prep, test_corpus, emmision_counts, vocab, states)\n",
        "\n",
        "print(\"Total accuracy is:\", round(acc, 2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total accuracy is: 0.89\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXDRReBhWyPw"
      },
      "source": [
        "# Part 2: Hidden Markov Models for POS\n",
        "Now you will build something more context specific. Concretely, you will be implementing a Hidden Markov Model (HMM) with a Viterbi decoder\n",
        "\n",
        "The HMM is one of the most commonly used algorithms in Natural Language Processing, and is a foundation to many deep learning techniques you will see in this specialization.\n",
        "\n",
        "In addition to parts-of-speech tagging, HMM is used in speech recognition, speech synthesis, etc.\n",
        "\n",
        "By completing this part of the assignment you should get a 95% accuracy on the same dataset you used in Part 1.\n",
        "\n",
        "The Markov Model contains a number of states and the probability of transition between those states.\n",
        "\n",
        "* In this case, the states are the parts-of-speech.\n",
        "* A Markov Model utilizes a transition matrix, A.\n",
        "* A Hidden Markov Model adds an observation or emission matrix B which describes the probability of a visible observation when we are in a particular state.\n",
        "* In this case, the emissions are the words in the corpus\n",
        "* The state, which is hidden, is the POS tag of that word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FnjH3_dXRw4"
      },
      "source": [
        "# Part 2.1 Generating Matrices\n",
        "Creating the 'A' transition probabilities matrix\n",
        "Now that you have your emission_counts, transition_counts, and tag_counts, you will start implementing the Hidden Markov Model.\n",
        "\n",
        "This will allow you to quickly construct the\n",
        "\n",
        "A transition probabilities matrix.\n",
        "and the B emission probabilities matrix.\n",
        "You will also use some smoothing when computing these matrices.\n",
        "\n",
        "Here is an example of what the A transition matrix would look like (it is simplified to 5 tags for viewing. It is 46x46 in this assignment.):\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th><strong>A</strong></th>\n",
        "<th>...</th>\n",
        "<th>RBS</th>\n",
        "<th>RP</th>\n",
        "<th>SYM</th>\n",
        "<th>TO</th>\n",
        "<th>UH</th>\n",
        "<th>...</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td><strong>RBS</strong></td>\n",
        "<td>...</td>\n",
        "<td>2.217069e-06</td>\n",
        "<td>2.217069e-06</td>\n",
        "<td>2.217069e-06</td>\n",
        "<td>0.008870</td>\n",
        "<td>2.217069e-06</td>\n",
        "<td>...</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>RP</strong></td>\n",
        "<td>...</td>\n",
        "<td>3.756509e-07</td>\n",
        "<td>7.516775e-04</td>\n",
        "<td>3.756509e-07</td>\n",
        "<td>0.051089</td>\n",
        "<td>3.756509e-07</td>\n",
        "<td>...</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>SYM</strong></td>\n",
        "<td>...</td>\n",
        "<td>1.722772e-05</td>\n",
        "<td>1.722772e-05</td>\n",
        "<td>1.722772e-05</td>\n",
        "<td>0.000017</td>\n",
        "<td>1.722772e-05</td>\n",
        "<td>...</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>TO</strong></td>\n",
        "<td>...</td>\n",
        "<td>4.477336e-05</td>\n",
        "<td>4.472863e-08</td>\n",
        "<td>4.472863e-08</td>\n",
        "<td>0.000090</td>\n",
        "<td>4.477336e-05</td>\n",
        "<td>...</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td><strong>UH</strong></td>\n",
        "<td>...</td>\n",
        "<td>1.030439e-05</td>\n",
        "<td>1.030439e-05</td>\n",
        "<td>1.030439e-05</td>\n",
        "<td>0.061837</td>\n",
        "<td>3.092348e-02</td>\n",
        "<td>...</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td>...</td>\n",
        "<td>...</td>\n",
        "<td>...</td>\n",
        "<td>...</td>\n",
        "<td>...</td>\n",
        "<td>...</td>\n",
        "<td>...</td>\n",
        "<td>...</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "\n",
        "Note that the matrix above was computed with smoothing.\n",
        "\n",
        "Each cell gives you the probability to go from one part of speech to another.\n",
        "\n",
        "In other words, there is a 4.47e-8 chance of going from parts-of-speech TO to RP.\n",
        "\n",
        "The sum of each row has to equal 1, because we assume that the next POS tag must be one of the available columns in the table.\n",
        "The smoothing was done as follows:\n",
        "\n",
        "$$ P(t_i | t_{i-1}) = \\frac{C(t_{i-1}, t_{i}) + \\alpha }{C(t_{i-1}) +\\alpha * N}\\tag{3}$$\n",
        "* $N$ is the total number of tags\n",
        "* $C(t_{i-1}, t_{i})$ is the count of the tuple (previous POS, current POS) in transition_counts dictionary.\n",
        "* $C(t_{i-1})$ is the count of the previous POS in the tag_counts dictionary.\n",
        "* $\\alpha$ is a smoothing parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlooxfNtXu3l"
      },
      "source": [
        "### Exercise 03\n",
        "**Instructions:** Implement the create_transition_matrix below for all tags. Your task is to output a matrix that computes equation 3 for each cell in matrix A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyO9vPoyYyu_"
      },
      "source": [
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    Alpha --> Number used for smoothing (something like 1e-3 or so, see equation)\n",
        "    tag_counts --> dictionary where key is a tag and value is the count\n",
        "    transition_counts --> dictionary where the keys are (prev_tag, tag) and the values are the counts\n",
        "\n",
        "    Outputs:\n",
        "    A --> Matrix of dimension (num_tags, num_tags) It's the one shown above\n",
        "    \"\"\"\n",
        "\n",
        "    # Get sorted list of unique tags and count them.\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "\n",
        "    # Initialize transition Matrix A with 0\n",
        "\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    # Get unique transition tuples (from, to)\n",
        "\n",
        "    trans_keys = set(transition_counts.keys())\n",
        "\n",
        "    # Go through the matrix updating each cell\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        for j in range(num_tags):\n",
        "            count = 0 # Initialize count of previous, current\n",
        "\n",
        "            # Get the tuple\n",
        "            key = (all_tags[i], all_tags[j]) # There's no double indexing in lists!\n",
        "            \n",
        "            if key in transition_counts: # If I have the tuple\n",
        "                count = transition_counts[key]\n",
        "\n",
        "            # Get the count of the previous tag\n",
        "\n",
        "            count_prev_tag = tag_counts[all_tags[i]]\n",
        "\n",
        "            # Smoothing\n",
        "\n",
        "            A[i, j] = (count + alpha)/(count_prev_tag + alpha * num_tags)\n",
        "\n",
        "    return A\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eh1grse_Lv68",
        "outputId": "ef3121ec-989d-442b-ff43-08390eccc319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "alpha = 0.001\n",
        "\n",
        "for i in range(46):\n",
        "    tag_counts.pop(i, None)\n",
        "\n",
        "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "\n",
        "print(f\"At row 0 col 0 we have {A[0, 0]:.9f}\")\n",
        "print(f\"At row 3 col 1 we have {A[3, 1]:.9f}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "At row 0 col 0 we have 0.000007040\n",
            "At row 3 col 1 we have 0.169101919\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}