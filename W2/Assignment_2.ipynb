{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6226LOwYDeYeRPBPy05B2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/NLP-SPEC-C2/blob/master/W2/Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx_bCmQGA5lf"
      },
      "source": [
        "#Assignment 2: Parts-of-Speech Tagging (POS)\n",
        "\n",
        "Welcome to the second assignment of Course 2 in the Natural Language Processing specialization. This assignment will develop skills in part-of-speech (POS) tagging, the process of assigning a part-of-speech tag (Noun, Verb, Adjective...) to each word in an input text. Tagging is difficult because some words can represent more than one part of speech at different times. They are Ambiguous. Let's look at the following example:\n",
        "\n",
        "* The whole team played well. [adverb]\n",
        "* You are doing well for yourself. [adjective]\n",
        "* Well, this assignment took me forever to complete. [interjection]\n",
        "* The well is dry. [noun]\n",
        "* Tears were beginning to well in her eyes. [verb]\n",
        "\n",
        "Distinguishing the parts-of-speech of a word in a sentence will help you better understand the meaning of a sentence. This would be critically important in search queries. Identifying the proper noun, the organization, the stock symbol, or anything similar would greatly improve everything ranging from speech recognition to search. By completing this assignment, you will:\n",
        "\n",
        "Learn how parts-of-speech tagging works\n",
        "* Compute the transition matrix A in a Hidden Markov Model\n",
        "* Compute the transition matrix B in a Hidden Markov Model\n",
        "* Compute the Viterbi algorithm\n",
        "* Compute the accuracy of your own model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4DRIGMH1HH7",
        "outputId": "77476934-9d04-42c2-b31c-09fe12bce861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download everything:\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_02-21.pos\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_24.pos\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/hmm_vocab.txt\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/test.words.txt\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/utils_pos.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-08 16:21:33--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_02-21.pos\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8279089 (7.9M) [text/plain]\n",
            "Saving to: ‘WSJ_02-21.pos’\n",
            "\n",
            "WSJ_02-21.pos       100%[===================>]   7.90M  15.9MB/s    in 0.5s    \n",
            "\n",
            "2020-10-08 16:21:34 (15.9 MB/s) - ‘WSJ_02-21.pos’ saved [8279089/8279089]\n",
            "\n",
            "--2020-10-08 16:21:34--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_24.pos\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 286063 (279K) [text/plain]\n",
            "Saving to: ‘WSJ_24.pos’\n",
            "\n",
            "WSJ_24.pos          100%[===================>] 279.36K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-10-08 16:21:35 (4.51 MB/s) - ‘WSJ_24.pos’ saved [286063/286063]\n",
            "\n",
            "--2020-10-08 16:21:35--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/hmm_vocab.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 196571 (192K) [text/plain]\n",
            "Saving to: ‘hmm_vocab.txt’\n",
            "\n",
            "hmm_vocab.txt       100%[===================>] 191.96K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-10-08 16:21:35 (4.27 MB/s) - ‘hmm_vocab.txt’ saved [196571/196571]\n",
            "\n",
            "--2020-10-08 16:21:35--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/test.words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180266 (176K) [text/plain]\n",
            "Saving to: ‘test.words.txt’\n",
            "\n",
            "test.words.txt      100%[===================>] 176.04K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2020-10-08 16:21:36 (3.97 MB/s) - ‘test.words.txt’ saved [180266/180266]\n",
            "\n",
            "--2020-10-08 16:21:36--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/utils_pos.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2455 (2.4K) [text/plain]\n",
            "Saving to: ‘utils_pos.py’\n",
            "\n",
            "utils_pos.py        100%[===================>]   2.40K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-08 16:21:36 (35.2 MB/s) - ‘utils_pos.py’ saved [2455/2455]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Xrpwsh1MC5",
        "outputId": "34b8fd71-6719-4294-b0bf-efb33f4c0a7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils_pos import get_word_tag, preprocess\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"Everything imported correctly\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything imported correctly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4IaRVTYBqgL"
      },
      "source": [
        "## Part 0: Data Sources\n",
        "This assignment will use two tagged data sets collected from the Wall Street Journal (WSJ).\n",
        "\n",
        "One data set (WSJ-2_21.pos) will be used for training.\n",
        "\n",
        "The other (WSJ-24.pos) for testing.\n",
        "\n",
        "The tagged training data has been preprocessed to form a vocabulary (hmm_vocab.txt).\n",
        "\n",
        "The words in the vocabulary are words from the training set that were used two or more times.\n",
        "\n",
        "The vocabulary is augmented with a set of 'unknown word tokens', described below.\n",
        "\n",
        "The training set will be used to create the emission, transmission and tag counts.\n",
        "\n",
        "The test set (WSJ-24.pos) is read in to create y.\n",
        "\n",
        "This contains both the test text and the true tag.\n",
        "The test set has also been preprocessed to remove the tags to form test_words.txt.\n",
        "\n",
        "This is read in and further processed to identify the end of sentences and handle words not in the vocabulary using functions provided in utils_pos.py.\n",
        "\n",
        "This forms the list prep, the preprocessed text used to test our POS taggers.\n",
        "A POS tagger will necessarily encounter words that are not in its datasets.\n",
        "\n",
        "To improve accuracy, these words are further analyzed during preprocessing to extract available hints as to their appropriate tag.\n",
        "\n",
        "For example, the suffix 'ize' is a hint that the word is a verb, as in 'final-ize' or 'character-ize'.\n",
        "\n",
        "A set of unknown-tokens, such as '--unk-verb--' or '--unk-noun--' will replace the unknown words in both the training and test corpus and will appear in the emission, transmission and tag data structures.\n",
        "\n",
        "Implementation note:\n",
        "\n",
        "* For python 3.6 and beyond, dictionaries retain the insertion order.\n",
        "* Furthermore, their hash-based lookup makes them suitable for rapid membership tests.\n",
        "  * If di is a dictionary, key in di will return True if di has a key key, else False.\n",
        "* The dictionary vocab will utilize these features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfQFZvVqCC0x",
        "outputId": "93eb0677-5888-4181-d7d7-bb96d50fdb78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load training corpus\n",
        "\n",
        "with open(\"WSJ_02-21.pos\", \"r\") as f:\n",
        "    training_corpus = f.readlines()\n",
        "\n",
        "print(f\"First 5 lines of the training corpus:\", training_corpus[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First 5 lines of the training corpus: ['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2zyMmg3CpLD",
        "outputId": "bdbb22e1-0c37-4dae-b6fd-0f7b3687ce05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read the vocabulary data, split by word (line) and save the vocab\n",
        "\n",
        "with open(\"hmm_vocab.txt\") as f:\n",
        "    voc_list = f.read().split(\"\\n\")\n",
        "\n",
        "print(f\"The first 5 words\", voc_list[0:5])\n",
        "\n",
        "print(f\"The last 5 words\", voc_list[-5:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first 5 words ['!', '#', '$', '%', '&']\n",
            "The last 5 words ['zones', 'zoning', '{', '}', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PFCxKFJDaH8"
      },
      "source": [
        "# Now make a dictionary of the words in vocab\n",
        "\n",
        "vocab = {}\n",
        "\n",
        "for i, word in enumerate(sorted(voc_list)):\n",
        "    vocab[word] = i\n",
        "\n",
        "print(\"Vocabulary dictionary. Key is the word and Value is a unique integer (ID), not a count.\")\n",
        "\n",
        "for k, v in vocab.items():\n",
        "    print(f\"{k}:{v}\")\n",
        "\n",
        "    if v > 10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYtFdg6JEsaM",
        "outputId": "b7127d2f-09c9-4506-e350-1b34d8795fae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load test corpus\n",
        "\n",
        "with open(\"WSJ_24.pos\", \"r\") as f:\n",
        "    test_corpus = f.readlines()\n",
        "\n",
        "print(f\"The first 5 lines of the test corpus:\", test_corpus[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first 5 lines of the test corpus: ['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGRai2XHFGGP"
      },
      "source": [
        "# Separate tags from test data\n",
        "\n",
        "_, prep = preprocess(vocab, \"test.words.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzywEMjSFcQU",
        "outputId": "31563c91-a5ea-4363-f288-be6eb89151d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(prep[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfsTEMQOFjxD"
      },
      "source": [
        "# Part 1: Parts-of-speech tagging\n",
        "\n",
        "## Part 1.1 - Training\n",
        "You will start with the simplest possible parts-of-speech tagger and we will build up to the state of the art.\n",
        "\n",
        "In this section, you will find the words that are not ambiguous.\n",
        "\n",
        "* For example, the word is is a verb and it is not ambiguous.\n",
        "* In the WSJ corpus, $86$% of the token are unambiguous (meaning they have only one tag)\n",
        "* About $14\\%$ are ambiguous (meaning that they have more than one tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q127qrxF4-r"
      },
      "source": [
        "Before you start predicting the tags of each word, you will need to compute a few dictionaries that will help you to generate the tables.\n",
        "\n",
        "Transition counts\n",
        "The first dictionary is the transition_counts dictionary which computes the number of times each tag happened next to another tag.\n",
        "This dictionary will be used to compute:$$P(t_i |t_{i-1}) \\tag{1}$$\n",
        "\n",
        "This is the probability of a tag at position $i$ given the tag at position $i-1$.\n",
        "\n",
        "In order for you to compute equation 1, you will create a transition_counts dictionary where\n",
        "\n",
        "The keys are (prev_tag, tag)\n",
        "The values are the number of times those two tags appeared in that order.\n",
        "Emission counts\n",
        "The second dictionary you will compute is the emission_counts dictionary. This dictionary will be used to compute:\n",
        "\n",
        "$$P(w_i|t_i)\\tag{2}$$\n",
        "In other words, you will use it to compute the probability of a word given its tag.\n",
        "\n",
        "In order for you to compute equation 2, you will create an emission_counts dictionary where\n",
        "\n",
        "The keys are (tag, word)\n",
        "The values are the number of times that pair showed up in your training set.\n",
        "Tag counts\n",
        "The last dictionary you will compute is the tag_counts dictionary.\n",
        "\n",
        "The key is the tag\n",
        "The value is the number of times each tag appeared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvQkpjRfF_pm"
      },
      "source": [
        "## Exercise 01\n",
        "**Instructions:** Write a program that takes in the training_corpus and returns the three dictionaries mentioned above transition_counts, emission_counts, and tag_counts.\n",
        "\n",
        "**emission_counts:** maps (tag, word) to the number of times it happened.\n",
        "\n",
        "**transition_counts:** maps (prev_tag, tag) to the number of times it has appeared.\n",
        "\n",
        "**tag_counts:** maps (tag) to the number of times it has occured.\n",
        "\n",
        "Implementation note: This routine utilises defaultdict, which is a subclass of dict.\n",
        "\n",
        "* A standard Python dictionary throws a KeyError if you try to access an item with a key that is not currently in the dictionary.\n",
        "* In contrast, the defaultdict will create an item of the type of the argument, in this case an integer with the default value of 0.\n",
        "* See defaultdict."
      ]
    }
  ]
}