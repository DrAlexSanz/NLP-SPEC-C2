{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNe9r+4KktCa6io2AvIEoUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/NLP-SPEC-C2/blob/master/W2/POS_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIzKTWJyMC_"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_02-21.pos\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/WSJ_24.pos\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/hmm_vocab.txt\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/test.words.txt\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C2/master/W2/utils_pos.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E027gH4krNTF"
      },
      "source": [
        "#Parts-of-Speech Tagging - First Steps:\n",
        "Working with text files, Creating a Vocabulary and Handling Unknown Words\n",
        "In this lecture notebook you will create a vocabulary from a tagged dataset and learn how to deal with words that are not present in this vocabulary when working with other text sources. Aside from this you will also learn how to:\n",
        "\n",
        "* read text files.\n",
        "* work with defaultdict.\n",
        "* work with string data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GkS8lUDsPqU"
      },
      "source": [
        "import string\n",
        "from collections import defaultdict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3AikHJ9sYao"
      },
      "source": [
        "## Read Text Data\n",
        "A tagged dataset taken from the Wall Street Journal is provided in the file WSJ_02-21.pos.\n",
        "\n",
        "To read this file you can use Python's context manager by using the with keyword and specifying the name of the file you wish to read. To actually save the contents of the file into memory you will need to use the readlines() method and store its return value in a variable.\n",
        "\n",
        "Python's context managers are great because you don't need to explicitly close the connection to the file, this is done under the hood:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6czZvBgNsdXj"
      },
      "source": [
        "with open(\"WSJ_02-21.pos\") as f:\n",
        "    lines = f.readlines()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eC7fk2lstTF",
        "outputId": "eacc8654-6e7b-4410-e4e5-f8ba3753f2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Check\n",
        "\n",
        "for i in range(5):\n",
        "    print(lines[i])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "In\tIN\n",
            "\n",
            "an\tDT\n",
            "\n",
            "Oct.\tNNP\n",
            "\n",
            "19\tCD\n",
            "\n",
            "review\tNN\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3joplHNtEiB"
      },
      "source": [
        "The previous is formatted, print only one and see the \\t and \\n:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T39ZBAxTtIb9",
        "outputId": "972419cd-7700-4745-df05-038891c14e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "lines[0]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'In\\tIN\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akS7pXdBthia"
      },
      "source": [
        "Indeed there is a tab between the word and the tag and a newline at the end of each line.\n",
        "\n",
        "### Creating a vocabulary\n",
        "Now that you understand how the dataset is structured, you will create a vocabulary out of it. A vocabulary is made up of every word that appeared at least 2 times in the dataset. For this, follow these steps:\n",
        "\n",
        "* Get only the words from the dataset\n",
        "* Use a defaultdict to count the number of times each word appears\n",
        "* Filter the dict to only include words that appeared at least 2 times\n",
        "* Create a list out of the filtered dict\n",
        "* Sort the list\n",
        "\n",
        "For step 1 you can use the fact that every word and tag are separated by a tab and that words always come first. Using list comprehension the words list can be created like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HoAQffPt3T5"
      },
      "source": [
        "words = [i.split(\"\\t\")[0] for i in lines]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoKGTyJZucYE"
      },
      "source": [
        "Step 2 can be done easily by leveraging defaultdict. In case you aren't familiar with defaultdicts they are a special kind of dictionaries that return the \"zero\" value of a type if you try to access a key that does not exist. Since you want the frequencies of words, you should define the defaultdict with a type of int.\n",
        "\n",
        "Now you don't need to worry about the case when the word is not present within the dictionary because getting the value for that key will simply return a zero. Isn't that cool?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dls9pmbHueXs"
      },
      "source": [
        "freq = defaultdict(int)\n",
        "\n",
        "for word in words:\n",
        "    freq[word] += 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsbzTfdEvX9p"
      },
      "source": [
        "Filtering the freq dictionary can be done using list comprehensions again (aren't they handy?). You should filter out words that appeared only once and also words that are just a newline character:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lm8CC11u7IF"
      },
      "source": [
        "vocab = [k for k, v in freq.items() if v > 1 and k != \"\\n\"]\n",
        "vocab.sort()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ErzR2GOvyax",
        "outputId": "0fdef8cb-ea02-428e-bdfe-ea8f13797bfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in range(4000, 4005):\n",
        "    print(vocab[i])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Early\n",
            "Earnings\n",
            "Earth\n",
            "Earthquake\n",
            "East\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzghXoj9wRzV"
      },
      "source": [
        "# Processing new text sources\n",
        "## Dealing with unknown words\n",
        "Now that you have a vocabulary, you will use it when processing new text sources. A new text will have words that do not appear in the current vocabulary. To tackle this, you can simply classify each new word as an unknown one, but you can do better by creating a function that tries to classify the type of each unknown word and assign it a corresponding unknown token.\n",
        "\n",
        "This function will do the following checks and return an appropriate token:\n",
        "\n",
        "* Check if the unknown word contains any character that is a digit\n",
        "  * return --unk_digit--\n",
        "* Check if the unknown word contains any punctuation character\n",
        "  * return --unk_punct--\n",
        "* Check if the unknown word contains any upper-case character\n",
        "  * return --unk_upper--\n",
        "* Check if the unknown word ends with a suffix that could indicate it is a noun, verb, adjective or adverb\n",
        "  * return --unk_noun--, --unk_verb--, --unk_adj--, --unk_adv-- respectively\n",
        "* If a word fails to fall under any condition then its token will be a plain --unk--.\n",
        "\n",
        "The conditions will be evaluated in the same order as listed here. So if a word contains a punctuation character but does not contain digits, it will fall under the second condition. To achieve this behaviour some if/elif statements can be used along with early returns.\n",
        "\n",
        "This function is implemented next. Notice that the any() function is being heavily used. It returns True if at least one of the cases it evaluates is True."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km9SeBz3wgZK"
      },
      "source": [
        "def assign_unk(word):\n",
        "\n",
        "    # Punctuation characters\n",
        "    punct = set(string.punctuation)\n",
        "\n",
        "    # Suffixes\n",
        "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
        "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
        "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
        "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
        "\n",
        "    # Find digits\n",
        "\n",
        "    if any(char.isdigit() for char in word):\n",
        "        return \"<UNK-DIGIT>\"\n",
        "\n",
        "    elif any(char in punct for char in word):\n",
        "        return \"<UNK-PUNCT>\"\n",
        "\n",
        "    elif any(char.isupper() for char in word):\n",
        "        return \"<UNK-UPPER>\" # Could be avoided by standarizing cases\n",
        "\n",
        "    # Check if word ends with any noun suffix\n",
        "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
        "        return \"<UNK-NOUN>\"\n",
        "\n",
        "    # Check if word ends with any verb suffix\n",
        "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
        "        return \"<UNK-VERB>\"\n",
        "\n",
        "    # Check if word ends with any adjective suffix\n",
        "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
        "        return \"<UNK-ADJ>\"\n",
        "\n",
        "    # Check if word ends with any adverb suffix\n",
        "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
        "        return \"<UNK-RB>\"\n",
        "    \n",
        "    # If none of the previous criteria is met, return plain unknown\n",
        "    else:\n",
        "        return \"<UNK>\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh2YSssPyFy3"
      },
      "source": [
        "## Getting the correct tag for a word\n",
        "All that is left is to implement a function that will get the correct tag for a particular word taking special considerations for unknown words. Since the dataset provides each word and tag within the same line and a word being known depends on the vocabulary used, these two elements should be arguments to this function.\n",
        "\n",
        "This function should check if a line is empty and if so, it should return a placeholder word and tag, --n-- and --s-- respectively.\n",
        "\n",
        "If not, it should process the line to return the correct word and tag pair, considering if a word is unknown in which scenario the function assign_unk() should be used.\n",
        "\n",
        "The function is implemented next. Notice That the split() method can be used without specifying the delimiter, in which case it will default to any whitespace."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no10Hu2hyVJN"
      },
      "source": [
        "def get_word_tag(line, vocab):\n",
        "\n",
        "  if not line.split():\n",
        "      word = \"Placeholder\"\n",
        "      tag = \"Placeholder\"\n",
        "  \n",
        "  else:\n",
        "      word, tag = line.split() # I separated by space\n",
        "\n",
        "      if word not in vocab:\n",
        "\n",
        "          word = assign_unk(word)\n",
        "\n",
        "  return word, tag\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKQ5L3CSzTrs",
        "outputId": "03bc4124-4b09-4580-c566-73522c3148ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_word_tag(\"\\n\", vocab)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Placeholder', 'Placeholder')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIIMnwaizuAo",
        "outputId": "a34ab085-805e-4f35-d401-d60f04e6b2a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_word_tag('In\\tIN\\n', vocab)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('In', 'IN')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRZekODMz1Z6",
        "outputId": "2969ca8d-ce8e-4b44-ecc6-d89dbb675bee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_word_tag('tardigrade\\tNN\\n', vocab) # UNK word but it's a noun"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<UNK>', 'NN')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wam_nObTz6rt",
        "outputId": "f560028b-76b9-47c9-9518-7c63edb66511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_word_tag('scrutinize\\tVB\\n', vocab)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('<UNK-VERB>', 'VB')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}