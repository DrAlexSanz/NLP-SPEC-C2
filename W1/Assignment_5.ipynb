{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZdeS/XZIKHL3c6CuXoeLq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DrAlexSanz/NLP-SPEC-C2/blob/master/W1/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nheRiEhsQ6xO"
      },
      "source": [
        "# Assignment 1: Sentiment with Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5vJmkBSROld"
      },
      "source": [
        "In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n",
        "\n",
        "Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will:\n",
        "\n",
        "* Understand how you can build/design a model using layers\n",
        "* Train a model using a training loop\n",
        "* Use a binary cross-entropy loss function\n",
        "* Compute the accuracy of your model\n",
        "* Predict using your own input\n",
        "\n",
        "As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization.\n",
        "\n",
        "Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library trax that we use for building and training models.\n",
        "Now we will show you how to compute the gradient of a certain function f by just using .grad(f).\n",
        "\n",
        "Trax source code can be found on Github: Trax\n",
        "The Trax code also uses the JAX library: JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FwigBZrRZ_4"
      },
      "source": [
        "## Part 1: Import libraries and try out Trax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZHfL6lDRbqG",
        "outputId": "a72fa49e-0e2d-42d8-d549-379c28b4b919"
      },
      "source": [
        "import os \n",
        "import random as rnd\n",
        "\n",
        "# Install trax\n",
        "\n",
        "!pip install sentencepiece==0.1.91\n",
        "!pip install trax\n",
        "\n",
        "# import relevant libraries\n",
        "import trax\n",
        "\n",
        "# set random seeds to make this notebook easier to replicate\n",
        "#trax.supervised.trainer_lib.init_random_number_generators(31)\n",
        "\n",
        "# import trax.fastmath.numpy\n",
        "import trax.fastmath.numpy as np\n",
        "\n",
        "# import trax.layers\n",
        "from trax import layers as tl\n",
        "\n",
        "# Download the utils file\n",
        "\n",
        "!wget https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C3/main/W1/utils.py\n",
        "\n",
        "# import Layer from the utils.py file\n",
        "from utils import Layer, load_tweets, process_tweet\n",
        "\n",
        "print(\"Imports OK\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 24.4MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 28.0MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 24.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 22.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61kB 16.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 71kB 16.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92kB 14.7MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 15.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 112kB 15.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122kB 15.8MB/s eta 0:00:01\r\u001b[K     |████                            | 133kB 15.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 143kB 15.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 153kB 15.8MB/s eta 0:00:01\r\u001b[K     |████▉                           | 163kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 174kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 184kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 194kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 215kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 225kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 235kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 245kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 256kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 266kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 276kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 286kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 296kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 307kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 317kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 327kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 337kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 348kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 358kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 368kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 378kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 389kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 399kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 409kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 419kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 430kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 440kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 450kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 460kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 471kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 481kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 491kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 501kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 512kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 522kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 532kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 542kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 552kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 563kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 573kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 583kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 593kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 604kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 614kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 624kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 634kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 645kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 655kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 665kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 675kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 686kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 696kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 706kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 716kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 727kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 737kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 747kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 757kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 768kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 778kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 788kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 798kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 808kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 819kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 829kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 839kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 849kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 860kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 870kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 880kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 890kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 901kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 911kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 921kB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 931kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 942kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 952kB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 962kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 972kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 983kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 993kB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0MB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.0MB 15.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.0MB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.0MB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1MB 15.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.1MB 15.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 15.8MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.91\n",
            "Collecting trax\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/1d/c0a3aeed127c26a0c3f0925fc9cc7278c272e52310eedfc322477e854972/trax-1.3.6-py2.py3-none-any.whl (468kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 11.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from trax) (1.15.0)\n",
            "Collecting funcsigs\n",
            "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.6/dist-packages (from trax) (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from trax) (1.18.5)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.6/dist-packages (from trax) (0.1.56+cuda101)\n",
            "Collecting tensorflow-text\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b2/2dbd90b93913afd07e6101b8b84327c401c394e60141c1e98590038060b3/tensorflow_text-2.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 53.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from trax) (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from trax) (1.4.1)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.6/dist-packages (from trax) (0.2.4)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.6/dist-packages (from trax) (4.0.1)\n",
            "Collecting t5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/39/a607d2450190af7675e4f77c5eff0cc9a83f82fe63fb396872ef2004106b/t5-0.7.1-py3-none-any.whl (172kB)\n",
            "\u001b[K     |████████████████████████████████| 174kB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from trax) (0.10.0)\n",
            "Requirement already satisfied: tensorflow<2.4,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->trax) (1.5.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.6/dist-packages (from jax->trax) (3.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (1.1.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.23.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (20.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (3.12.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.24.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.3.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets->trax) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.22.2.post1)\n",
            "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/8b/553deb763ce8d00afb17debab7cb14a87b209cd4c6f0e8ecfc8d884cb12a/mesh_tensorflow-0.1.17-py3-none-any.whl (342kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from t5->trax) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.1.4)\n",
            "Collecting transformers>=2.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 50.7MB/s \n",
            "\u001b[?25hCollecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/c4/8e948f601a4f9609e8b2b58f31966cb13cf17b940b82aa3e767f01c42c52/sacrebleu-1.4.14-py3-none-any.whl (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from t5->trax) (0.1.91)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Collecting tfds-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/16/5d772c70a058e76e7c9b6ba72fdfda315a81df16e2cfd2e31f1c3bcb1564/tfds_nightly-4.1.0.dev202011220107-py3-none-any.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.6/dist-packages (from t5->trax) (2.8.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from t5->trax) (1.7.0+cu101)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.12.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.6.3)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.33.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.35.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets->trax) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow-datasets->trax) (3.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets->trax) (50.3.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets->trax) (1.52.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->t5->trax) (0.17.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->t5->trax) (2.8.1)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 47.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 44.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.7.0->t5->trax) (3.0.12)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from tfds-nightly->t5->trax) (3.7.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.17.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.7.0->t5->trax) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.7.0->t5->trax) (7.1.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (2.0.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow<2.4,>=2.3.0->tensorflow-text->trax) (0.4.8)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=9f9dfe561624647b7596ab76493a227c0dd2a87157e3e765492cd2c0d4a99ab2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: funcsigs, tensorflow-text, mesh-tensorflow, tokenizers, sacremoses, transformers, portalocker, sacrebleu, rouge-score, tfds-nightly, t5, trax\n",
            "Successfully installed funcsigs-1.0.2 mesh-tensorflow-0.1.17 portalocker-2.0.0 rouge-score-0.0.4 sacrebleu-1.4.14 sacremoses-0.0.43 t5-0.7.1 tensorflow-text-2.3.0 tfds-nightly-4.1.0.dev202011220107 tokenizers-0.9.3 transformers-3.5.1 trax-1.3.6\n",
            "--2020-11-22 20:01:19--  https://raw.githubusercontent.com/DrAlexSanz/NLP-SPEC-C3/main/W1/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2789 (2.7K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "utils.py            100%[===================>]   2.72K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-22 20:01:19 (51.6 MB/s) - ‘utils.py’ saved [2789/2789]\n",
            "\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Imports OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeUgtqZ5i1fw"
      },
      "source": [
        "### Since I imported trax's version of numpy I can create vectors directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E4lMO1MjGIi",
        "outputId": "2e9df6d7-c355-4159-d23a-e8cad1734506"
      },
      "source": [
        "a = np.array((5., 2.))\n",
        "\n",
        "type(a) # Notice it's not a np array but a jax DeviceArray."
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "jax.interpreters.xla.DeviceArray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaDTH7ohjd4T",
        "outputId": "e39da7ca-46e4-45a8-9bd5-5775625c4c91"
      },
      "source": [
        "# Now do a function with the same array\n",
        "\n",
        "def f(x):\n",
        "\n",
        "    return(x**2)\n",
        "\n",
        "print(f\"f(a) = {f(a)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f(a) = [25.  4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdWRmkH3lV1J",
        "outputId": "8702dd1e-e450-48c8-d4db-cb9d8c8659e8"
      },
      "source": [
        "# And now the derivative (2x)\n",
        "\n",
        "grad_f = trax.fastmath.grad(fun = f) # grad only takes scalar arguments\n",
        "type(grad_f)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "function"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "GhxAa0mcljy6",
        "outputId": "1611fc97-022b-446a-c626-3664afff1659"
      },
      "source": [
        "# grad_f(a)\n",
        "b = 13.0\n",
        "grad_b = grad_f(b)\n",
        "\n",
        "display(grad_b)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DeviceArray(26., dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVb6aFN4mNI2"
      },
      "source": [
        "# Part 2: Importing the data\n",
        "\n",
        "## 2.1 Loading in the data\n",
        "Import the data set.\n",
        "\n",
        "* You may recognize this from earlier assignments in the specialization.\n",
        "* Details of process_tweet function are available in utils.py file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkv1y-RKmTVN"
      },
      "source": [
        "import numpy as np # Let's go back to the usual thing."
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONT7qtvEmcgk",
        "outputId": "77cfa180-81fd-424a-c4c8-5662785bb773"
      },
      "source": [
        "all_pos_tweets, all_neg_tweets = load_tweets()\n",
        "\n",
        "print(\"Number of positive tweets\", len(all_pos_tweets))\n",
        "print(\"Number of negative tweets\", len(all_neg_tweets))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive tweets 5000\n",
            "Number of negative tweets 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x_VX39lno33"
      },
      "source": [
        "## Now I'll create train and validation sets\n",
        "\n",
        "* Shuffle the tweets if they are not randomly sorted.\n",
        "* Split the positive in train-test (80-20 because I can).\n",
        "* Add labels (1 positive, 0 negative).\n",
        "* Check it all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-C3PiPFn_Aa",
        "outputId": "a75a1575-c925-46df-ce62-e03a0a7bc330"
      },
      "source": [
        "train_pos = all_pos_tweets[:4000]\n",
        "test_pos = all_pos_tweets[4000:]\n",
        "\n",
        "train_neg = all_neg_tweets[:4000]\n",
        "test_neg = all_neg_tweets[4000:]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "\n",
        "print(\"Length of train_pos:\", len(train_pos))\n",
        "print(\"Length of train_neg:\", len(train_neg))\n",
        "\n",
        "print(\"Length of test_pos:\", len(test_pos))\n",
        "print(\"Length of test_neg:\", len(test_neg))\n",
        "\n",
        "print(\"Length of train_x:\", len(train_x))\n",
        "print(\"Length of train_y:\", len(train_y))\n",
        "\n",
        "print(\"First 5 values of tags\", train_y[0:5])\n",
        "print(\"Last 5 values of tags\", train_y[-5:])\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of train_pos: 4000\n",
            "Length of train_neg: 4000\n",
            "Length of test_pos: 1000\n",
            "Length of test_neg: 1000\n",
            "Length of train_x: 8000\n",
            "Length of train_y: 8000\n",
            "First 5 values of tags [1. 1. 1. 1. 1.]\n",
            "Last 5 values of tags [0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLJsTGK_qid-"
      },
      "source": [
        "### Preprocess the tweets to clean them. I have a function but in any case I'm used to this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehLfODQrqrT_",
        "outputId": "a09de5d3-a15e-4e1f-8a0f-ff28d8758fb7"
      },
      "source": [
        "# This function only processes one tweet. I'll call it in a loop or a list comprehension\n",
        "\n",
        "print(\"The first positive tweet is:\", all_pos_tweets[0])\n",
        "\n",
        "clean_tweet = process_tweet(all_pos_tweets[0])\n",
        "\n",
        "print(\"The clean tweet is:\", clean_tweet) # Notice it removes all the twitter handles and the \"#\" symbol. It also tokenizes and stems the words."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The first positive tweet is: #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "The clean tweet is: ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NrQh_qYrqfo"
      },
      "source": [
        "# 2.2 Building the vocabulary\n",
        "Now build the vocabulary.\n",
        "\n",
        "* Map each word in each tweet to an integer (an \"index\").\n",
        "* The following code does this for you, but please read it and understand what it's doing.\n",
        "* Note that you will build the vocabulary based on the training data.\n",
        "* To do so, you will assign an index to everyword by iterating over your training set.\n",
        "* The vocabulary will also include some special tokens\n",
        "\n",
        "* <--PAD--> padding\n",
        "* <--END-->: end of line\n",
        "* <--UNK-->: a token representing any word that is not in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXor3QX7vL1L"
      },
      "source": [
        "# Start with the padding, end and UNK\n",
        "\n",
        "vocab = {\"<PAD>\": 0, \"<END>\": 1, \"<UNK>\": 2}\n",
        "\n",
        "# Keep in mind, the vocabulary is only with the training data!!\n",
        "\n",
        "for tweet in train_x:\n",
        "    processed_tweet = process_tweet(tweet)\n",
        "\n",
        "    for word in processed_tweet:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = len(vocab) # len vocab changes with every new word\n",
        "\n",
        "print(\"Total words:\", len(vocab))\n",
        "\n",
        "#display(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2UNjeh2NLx"
      },
      "source": [
        "## Exercise 01\n",
        "Instructions: Write a program tweet_to_tensor that takes in a tweet and converts it to an array of numbers. You can use the Vocab dictionary you just found to help create the tensor.\n",
        "\n",
        "* Use the vocab_dict parameter and not a global variable.\n",
        "* Do not hard code the integer value for the __UNK__ token.\n",
        "* Map each word in tweet to corresponding token in 'Vocab'\n",
        "* Use Python's Dictionary.get(key,value) so that the function returns a default value if the key is not found in the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2s9d7XQ2eLX"
      },
      "source": [
        "def tweet_to_tensor(tweet, vocab_dict, unk_token = \"<UNK>\"):\n",
        "    \"\"\"\n",
        "    Take a tweet (tokens) and return a tensor (numbers) basically, translate from keys to values\n",
        "    Inputs:\n",
        "      Tweet: a clean tweet\n",
        "      vocab_dict: the vocabulary, word and index\n",
        "    Output:\n",
        "      tensor_l: a vector (list) with the indices of the words\n",
        "    \"\"\"\n",
        "    word_list = process_tweet(tweet)\n",
        "\n",
        "    unk_ID = vocab_dict[unk_token]\n",
        "    tensor_l = []\n",
        "\n",
        "    for word in word_list:\n",
        "      if word in vocab_dict:\n",
        "        word_ID = vocab_dict[word]\n",
        "        tensor_l.append(word_ID)\n",
        "      else:\n",
        "        tensor_l.append(unk_ID)\n",
        "\n",
        "    return tensor_l\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAs0CoVb4HPL",
        "outputId": "24ce1466-ab51-4fc1-f014-234472beec61"
      },
      "source": [
        "print(\"Tweet is:\", test_pos[0])\n",
        "tensor_tweet = tweet_to_tensor(test_pos[0], vocab)\n",
        "print(\"Corresponding tensor is:\", tensor_tweet)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tweet is: Bro:U wan cut hair anot,ur hair long Liao bo\n",
            "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
            "Bro:LOL Sibei xialan\n",
            "Corresponding tensor is: [1065, 136, 479, 2351, 745, 8146, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41fpcfuQ43GL",
        "outputId": "55ac2863-0b5a-4a6f-bba2-a40732834871"
      },
      "source": [
        "#Thorough checks\n",
        "\n",
        "# test tweet_to_tensor\n",
        "\n",
        "def test_tweet_to_tensor():\n",
        "    test_cases = [\n",
        "        \n",
        "        {\n",
        "            \"name\":\"simple_test_check\",\n",
        "            \"input\": [test_pos[1], vocab],\n",
        "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
        "            \"error\":\"The function gives bad output for test_pos[1]. Test failed\"\n",
        "        },\n",
        "        {\n",
        "            \"name\":\"datatype_check\",\n",
        "            \"input\":[test_pos[1], vocab],\n",
        "            \"expected\":type([]),\n",
        "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
        "        },\n",
        "        {\n",
        "            \"name\":\"without_unk_check\",\n",
        "            \"input\":[test_pos[1], vocab],\n",
        "            \"expected\":6,\n",
        "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
        "        }\n",
        "    ]\n",
        "    count = 0\n",
        "    for test_case in test_cases:\n",
        "        \n",
        "        try:\n",
        "            if test_case['name'] == \"simple_test_check\":\n",
        "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
        "                count += 1\n",
        "            if test_case['name'] == \"datatype_check\":\n",
        "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
        "                count += 1\n",
        "            if test_case['name'] == \"without_unk_check\":\n",
        "                assert None not in tweet_to_tensor(*test_case['input'])\n",
        "                count += 1\n",
        "                \n",
        "            \n",
        "            \n",
        "        except:\n",
        "            print(test_case['error'])\n",
        "    if count == 3:\n",
        "        print(\"\\033[92m All tests passed\")\n",
        "    else:\n",
        "        print(count,\" Tests passed out of 3\")\n",
        "test_tweet_to_tensor()\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}